name: Plagiarism Check

on:
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  check-plagiarism:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Java
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 colorama

      - name: Download JPlag
        run: |
          curl -L -o jplag.jar https://github.com/jplag/JPlag/releases/download/v3.0.0/jplag-3.0.0-jar-with-dependencies.jar

      - name: Detect solution folders
        id: detect-folders
        run: |
          # Get all directories (excluding .git and .github)
          FOLDERS=$(find . -maxdepth 1 -type d -not -path "*/\\.*" -not -path "." | sort)
          echo "Found solution folders: $FOLDERS"
          echo "solution_folders=$FOLDERS" >> $GITHUB_OUTPUT

      - name: Check plagiarism with multiple tools
        id: plagiarism-check
        run: |
          # Create a Python script for enhanced plagiarism checking
          cat > check_plagiarism.py << 'EOL'
          import os
          import sys
          import re
          import json
          import subprocess
          from pathlib import Path
          from bs4 import BeautifulSoup
          import requests
          from colorama import Fore, Style, init
          import collections
          import shutil
          import hashlib
          import difflib

          # Initialize colorama
          init()

          # Get all directories in current path (excluding .git and .github)
          solution_folders = [d for d in Path('.').iterdir() 
                              if d.is_dir() and not d.name.startswith('.')]
          
          print(f"Found {len(solution_folders)} solution folders: {[f.name for f in solution_folders]}")
          
          # Define languages to check for with correct JPlag language options
          language_configs = [
              {"name": "python", "extensions": [".py"], "jplag_lang": "python3"},
              {"name": "java", "extensions": [".java"], "jplag_lang": "java"},
              {"name": "javascript", "extensions": [".js", ".jsx"], "jplag_lang": "text"},  # Use text mode for JS
              {"name": "typescript", "extensions": [".ts", ".tsx"], "jplag_lang": "text"},  # Use text mode for TS
              {"name": "cpp", "extensions": [".cpp", ".hpp", ".cc", ".h"], "jplag_lang": "cpp"},
              {"name": "c", "extensions": [".c", ".h"], "jplag_lang": "cpp"},  # Use cpp for C
              {"name": "html", "extensions": [".html", ".htm"], "jplag_lang": "text"},
              {"name": "css", "extensions": [".css", ".scss", ".sass"], "jplag_lang": "text"}
          ]

          # Dictionary to track plagiarism percentages
          file_plagiarism = collections.defaultdict(list)
          team_plagiarism = collections.defaultdict(list)
          results = []

          # Create results directory
          results_dir = Path("plagiarism_results")
          if results_dir.exists():
              shutil.rmtree(results_dir)
          results_dir.mkdir()

          # Function for custom code similarity check
          def check_similarity(file1, file2):
              try:
                  with open(file1, 'r', encoding='utf-8', errors='ignore') as f1, \
                       open(file2, 'r', encoding='utf-8', errors='ignore') as f2:
                      content1 = f1.read()
                      content2 = f2.read()
                      
                      # Remove comments and normalize whitespace to reduce false positives
                      content1 = re.sub(r'\/\/.*|\/\*[\s\S]*?\*\/|#.*', '', content1)
                      content2 = re.sub(r'\/\/.*|\/\*[\s\S]*?\*\/|#.*', '', content2)
                      content1 = re.sub(r'\s+', ' ', content1).strip()
                      content2 = re.sub(r'\s+', ' ', content2).strip()
                      
                      # Skip empty files after comment removal
                      if not content1 or not content2:
                          return 0
                      
                      # Use Python's difflib to compute similarity
                      similarity = difflib.SequenceMatcher(None, content1, content2).ratio() * 100
                      return similarity
              except Exception as e:
                  print(f"{Fore.RED}Error comparing {file1} and {file2}: {e}{Style.RESET_ALL}")
                  return 0

          # Run plagiarism check for each language
          for lang_config in language_configs:
              lang_name = lang_config["name"]
              lang_extensions = lang_config["extensions"]
              jplag_lang = lang_config["jplag_lang"]
              
              # Find all files of the given language
              language_files = []
              for folder in solution_folders:
                  for ext in lang_extensions:
                      language_files.extend(list(Path(folder).glob(f"**/*{ext}")))
              
              file_count = len(language_files)
              
              if file_count < 2:
                  print(f"{Fore.YELLOW}Not enough {lang_name} files to compare ({file_count} found). Skipping.{Style.RESET_ALL}")
                  continue
              
              print(f"{Fore.CYAN}Checking for plagiarism in {file_count} {lang_name} files...{Style.RESET_ALL}")
              
              # Create output directory for this language
              output_dir = results_dir / lang_name
              output_dir.mkdir(exist_ok=True)
              
              # Method 1: Use JPlag if we have multiple solution folders
              if len(solution_folders) > 1:
                  try:
                      # Run JPlag with correct parameters (no % sign in threshold)
                      cmd = [
                          "java", "-jar", "jplag.jar",
                          "-l", jplag_lang,
                          "-r", str(output_dir),
                          "-m", "0",  # Minimum match length as integer, not percentage
                          "."  # Search in current directory
                      ]
                      
                      # Add language-specific exclude patterns for other languages
                      for exclude_lang in language_configs:
                          if exclude_lang["name"] != lang_name:
                              for ext in exclude_lang["extensions"]:
                                  cmd.extend(["-x", f"**/*{ext}"])
                      
                      # Exclude .git and .github directories
                      cmd.extend(["-x", ".git/**", "-x", ".github/**"])
                      
                      print(f"Running JPlag: {' '.join(cmd)}")
                      process = subprocess.run(cmd, capture_output=True, text=True)
                      
                      if process.returncode != 0:
                          print(f"{Fore.RED}Error running JPlag for {lang_name}:{Style.RESET_ALL}")
                          print(process.stderr)
                      else:
                          print(f"{Fore.GREEN}JPlag completed for {lang_name}{Style.RESET_ALL}")
                          
                          # Parse the results
                          index_file = output_dir / "index.html"
                          if index_file.exists():
                              # Parse the HTML report
                              with open(index_file, "r", encoding="utf-8") as f:
                                  soup = BeautifulSoup(f.read(), 'html.parser')
                              
                              # Extract matches from the table
                              table = soup.find('table')
                              if table:
                                  rows = table.find_all('tr')[1:]  # Skip header row
                                  
                                  for row in rows:
                                      cells = row.find_all('td')
                                      if len(cells) >= 3:
                                          similarity_cell = cells[0]
                                          similarity_text = similarity_cell.get_text(strip=True)
                                          similarity_value = float(similarity_text.replace('%', ''))
                                          
                                          # Get team names from the comparison
                                          team_links = cells[1:3]
                                          team1 = team_links[0].get_text(strip=True).split('/')[0]
                                          team2 = team_links[1].get_text(strip=True).split('/')[0]
                                          
                                          # Get link to the detailed comparison
                                          comparison_link = team_links[0].find('a')['href'] if team_links[0].find('a') else ""
                                          
                                          # Add to team plagiarism scores
                                          team_plagiarism[team1].append(similarity_value)
                                          team_plagiarism[team2].append(similarity_value)
                                          
                                          # Add to results
                                          results.append({
                                              'language': lang_name,
                                              'team1': team1,
                                              'team2': team2,
                                              'similarity': f"{similarity_value}%",
                                              'match_url': f"{lang_name}/{comparison_link}",
                                              'tool': 'JPlag'
                                          })
                              else:
                                  print(f"{Fore.YELLOW}No JPlag results found for {lang_name}.{Style.RESET_ALL}")
                  except Exception as e:
                      print(f"{Fore.RED}Error in JPlag processing for {lang_name}: {e}{Style.RESET_ALL}")
              
              # Method 2: Custom similarity check (works for both single and multiple folders)
              try:
                  print(f"{Fore.CYAN}Running custom similarity check for {lang_name}...{Style.RESET_ALL}")
                  
                  # Group files by solution folder
                  folder_files = collections.defaultdict(list)
                  for file in language_files:
                      folder_name = file.parts[0]  # Get the top-level folder name
                      folder_files[folder_name].append(file)
                  
                  # Store results for file comparisons
                  custom_results = []
                  
                  # For multiple folders: compare files between different folders
                  if len(solution_folders) > 1:
                      for folder1, files1 in folder_files.items():
                          for folder2, files2 in folder_files.items():
                              if folder1 >= folder2:  # Skip comparing a folder with itself and avoid duplicate comparisons
                                  continue
                              
                              # Compare each file from folder1 with each file from folder2
                              for file1 in files1:
                                  for file2 in files2:
                                      similarity = check_similarity(file1, file2)
                                      
                                      if similarity >= 30:  # Only record significant matches
                                          custom_results.append({
                                              'file1': str(file1),
                                              'file2': str(file2),
                                              'similarity': similarity,
                                              'folder1': folder1,
                                              'folder2': folder2
                                          })
                  # For single folder: compare files within the same folder
                  else:
                      for folder, files in folder_files.items():
                          # Compare files within the same folder
                          for i, file1 in enumerate(files):
                              for j, file2 in enumerate(files):
                                  if i >= j:  # Skip comparing a file with itself and avoid duplicate comparisons
                                      continue
                                      
                                  similarity = check_similarity(file1, file2)
                                  
                                  if similarity >= 30:  # Only record significant matches
                                      # Extract relative paths for more meaningful comparison
                                      rel_path1 = "/".join(Path(file1).parts[1:])
                                      rel_path2 = "/".join(Path(file2).parts[1:])
                                      
                                      custom_results.append({
                                          'file1': str(file1),
                                          'file2': str(file2),
                                          'rel_path1': rel_path1,
                                          'rel_path2': rel_path2,
                                          'similarity': similarity,
                                          'folder': folder
                                      })
                  
                  # Sort results by similarity
                  custom_results.sort(key=lambda x: x['similarity'], reverse=True)
                  
                  # Save the detailed results
                  custom_results_file = output_dir / "custom_results.json"
                  with open(custom_results_file, 'w') as f:
                      json.dump(custom_results, f, indent=2)
                  
                  # Add to overall results
                  if custom_results:
                      if len(solution_folders) > 1:
                          # For multiple folders: group by folder pairs
                          folder_pairs = {}
                          for res in custom_results:
                              pair = (res['folder1'], res['folder2'])
                              if pair not in folder_pairs or res['similarity'] > folder_pairs[pair]:
                                  folder_pairs[pair] = res['similarity']
                          
                          for (folder1, folder2), similarity in folder_pairs.items():
                              # Add to team plagiarism scores
                              team_plagiarism[folder1].append(similarity)
                              team_plagiarism[folder2].append(similarity)
                              
                              # Add to overall results
                              results.append({
                                  'language': lang_name,
                                  'team1': folder1,
                                  'team2': folder2,
                                  'similarity': f"{similarity:.2f}%",
                                  'match_url': f"{lang_name}/custom_results.json",
                                  'tool': 'Custom'
                              })
                      else:
                          # For single folder: group by file pairs
                          # In this case we'll use relative file paths as identifiers
                          for res in custom_results:
                              # Add results using relative paths for clearer reporting
                              results.append({
                                  'language': lang_name,
                                  'team1': res['rel_path1'],
                                  'team2': res['rel_path2'], 
                                  'similarity': f"{res['similarity']:.2f}%",
                                  'match_url': f"{lang_name}/custom_results.json",
                                  'tool': 'Custom'
                              })
                              
                              # Add to file plagiarism scores
                              file_plagiarism[res['rel_path1']].append(res['similarity'])
                              file_plagiarism[res['rel_path2']].append(res['similarity'])
                          
                      print(f"{Fore.GREEN}Found {len(custom_results)} matches in custom check for {lang_name}{Style.RESET_ALL}")
              except Exception as e:
                  print(f"{Fore.RED}Error in custom similarity check for {lang_name}: {e}{Style.RESET_ALL}")

          # Calculate the maximum plagiarism percentage for reporting
          plagiarism_max = {}
          
          # For multiple folders, report by team/folder
          if len(solution_folders) > 1:
              for team, scores in team_plagiarism.items():
                  if scores:
                      # Use the maximum plagiarism score found for the team
                      plagiarism_max[team] = max(scores)
                  else:
                      plagiarism_max[team] = 0
          # For single folder, report by file
          else:
              for file_path, scores in file_plagiarism.items():
                  if scores:
                      # Use the maximum plagiarism score found for the file
                      plagiarism_max[file_path] = max(scores)
                  else:
                      plagiarism_max[file_path] = 0

          # Save the results as JSON
          with open("plagiarism_results.json", "w") as f:
              json.dump({
                  'detailed_results': results,
                  'plagiarism_max': plagiarism_max,
                  'is_single_folder': len(solution_folders) == 1
              }, f, indent=2)

          # Create a result URL file for GitHub Actions
          with open("plagiarism_result_url.txt", "w") as f:
              f.write("plagiarism_results/")

          # Print results in the console
          if results:
              print(f"\n{Fore.RED}Found {len(results)} potential plagiarism cases:{Style.RESET_ALL}")
              for result in results:
                  print(f"{Fore.YELLOW}Language: {result['language']} - Match: {result['similarity']} between {result['team1']} and {result['team2']} (Tool: {result['tool']}){Style.RESET_ALL}")
              
              print(f"\n{Fore.CYAN}Plagiarism Summary:{Style.RESET_ALL}")
              for item, score in plagiarism_max.items():
                  status = f"{Fore.GREEN}Low" if score < 30 else f"{Fore.YELLOW}Medium" if score < 70 else f"{Fore.RED}High"
                  print(f"{item}: {status} ({score}%){Style.RESET_ALL}")
          else:
              print(f"\n{Fore.GREEN}No plagiarism detected!{Style.RESET_ALL}")
          EOL

          # Run the plagiarism check script
          python check_plagiarism.py || echo "Plagiarism check script execution failed"
          
          # Clean up temporary files
          rm -f check_plagiarism.py

      - name: Check for AI-generated code
        run: |
          # Create a Python script to detect AI-generated code
          cat > detect_ai_code.py << 'EOL'
          import os
          import re
          from pathlib import Path

          def detect_ai_patterns(file_content):
              """Detect patterns that might indicate AI-generated code."""
              ai_patterns = [
                  r"# Generated by ChatGPT",  # Example AI comment
                  r"# AI-generated code",  # Example AI comment
                  r"def .*_ai_.*\(.*\):",  # Function names with '_ai_'
                  r"Generated by OpenAI",  # Common AI tool signature
                  r"This (code|file) was generated by",  # Generic AI generation comment
                  r"<!-- Generated by AI -->",  # HTML comment for AI generation
                  r"\/\* Generated by AI \*\/",  # CSS/JS comment
                  r"// Generated by"  # JS/TS comment
              ]

              for pattern in ai_patterns:
                  if re.search(pattern, file_content, re.IGNORECASE):
                      return True
              return False

          # Scan all solution files
          suspicious_files = []
          for file_path in Path('.').rglob('*.*'):
              if file_path.suffix in ['.py', '.java', '.js', '.jsx', '.ts', '.tsx', '.cpp', '.c', '.h', '.html', '.htm', '.css', '.scss', '.sass']:
                  try:
                      with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                          content = f.read()
                          if detect_ai_patterns(content):
                              suspicious_files.append(str(file_path))
                  except Exception as e:
                      print(f"Error reading {file_path}: {e}")

          # Output results
          if suspicious_files:
              print("\n[WARNING] Potential AI-generated code detected in the following files:")
              for file in suspicious_files:
                  print(f"- {file}")
              with open("ai_detection_results.txt", "w") as f:
                  f.write("\n".join(suspicious_files))
          else:
              print("\n[INFO] No AI-generated code detected.")
          EOL

          # Run the AI detection script
          python detect_ai_code.py || echo "AI detection script execution failed"
          
          # Clean up the temporary script file
          rm -f detect_ai_code.py

      - name: Upload AI detection results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-detection-results
          path: ai_detection_results.txt
          if-no-files-found: ignore

      - name: Upload plagiarism results
        if: always()
        run: |
          # Create empty files if they don't exist to prevent upload failures
          mkdir -p plagiarism_results
          if [ ! -f "plagiarism_result_url.txt" ]; then
            echo "No results available" > plagiarism_result_url.txt
          fi
          if [ ! -f "plagiarism_results.json" ]; then
            echo '{"detailed_results":[],"plagiarism_max":{}}' > plagiarism_results.json
          fi

      - name: Upload results as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: plagiarism-results
          path: |
            plagiarism_result_url.txt
            plagiarism_results.json
            plagiarism_results/
          if-no-files-found: warn

      - name: Update README with plagiarism results
        if: success()
        run: |
          # Create a Python script to update the README
          cat > update_readme.py << 'EOL'
          import json
          import re
          
          # Read the plagiarism results
          try:
              with open('plagiarism_results.json', 'r') as f:
                  data = json.load(f)
                  plagiarism_max = data.get('plagiarism_max', {})
                  is_single_folder = data.get('is_single_folder', False)
          except FileNotFoundError:
              print("No plagiarism results found. Skipping README update.")
              exit(0)
          
          # Read the current README.md
          with open('README.md', 'r') as f:
              readme_content = f.read()
          
          # Check if plagiarism results section already exists
          plagiarism_section = "## 📊 Plagiarism Results\n"
          plagiarism_table = "| Item | Plagiarism Percentage |\n|------|----------------------|\n"
          
          # Sort items by plagiarism percentage (highest first)
          sorted_items = sorted(plagiarism_max.items(), key=lambda x: x[1], reverse=True)
          
          # Generate the plagiarism table
          for item, score in sorted_items:
              status = "✅ Low" if score < 30 else "⚠️ Medium" if score < 70 else "🚫 High"
              plagiarism_table += f"| {item} | {status} ({score}%) |\n"
          
          # Generate the complete plagiarism section
          complete_section = f"{plagiarism_section}{plagiarism_table}\n"
          
          # Check if the section already exists and update it
          if "## 📊 Plagiarism Results" in readme_content:
              # Replace the existing section
              pattern = r"## 📊 Plagiarism Results\n(\|.*\n)+"
              readme_content = re.sub(pattern, complete_section, readme_content)
          else:
              # Add the section before "Happy Coding" section
              if "## 🚀 Happy Coding!" in readme_content:
                  readme_content = readme_content.replace("## 🚀 Happy Coding!", f"{complete_section}## 🚀 Happy Coding!")
              else:
                  # If "Happy Coding" section doesn't exist, add to the end
                  readme_content += f"\n{complete_section}"
          
          # Write the updated README
          with open('README.md', 'w') as f:
              f.write(readme_content)
          
          print("README.md updated with plagiarism results successfully!")
          EOL
          
          # Run the update script
          python update_readme.py
          
          # Clean up the temporary script file
          rm -f update_readme.py

      - name: Update README with all logs
        if: always()
        run: |
          # Append all logs to the README
          echo "## 📝 Workflow Logs" >> README.md
          echo "### Solution Folders Detected" >> README.md
          echo "$FOLDERS" >> README.md

          if [ -f "plagiarism_results.json" ]; then
            echo "### Plagiarism Results" >> README.md
            cat plagiarism_results.json >> README.md
          fi

          if [ -f "ai_detection_results.txt" ]; then
            echo "### AI Detection Results" >> README.md
            cat ai_detection_results.txt >> README.md
          fi

          echo "Logs appended to README.md successfully!"

      - name: Commit README changes
        if: success()
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Clean up any leftover files before committing
          rm -f jplag.jar
          rm -f ai_detection_results.txt 
          
          git add README.md
          git commit -m "Update README with plagiarism results [skip ci]" || echo "No changes to commit"
          
          # Use the GitHub token with explicit authentication
          echo "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com" > ~/.git-credentials
          git config --global credential.helper store
          git push origin HEAD:${GITHUB_REF_NAME}

      - name: Comment on PR
        if: github.event_name == 'pull_request' && github.actor != 'github-actions[bot]'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            try {
              if (fs.existsSync('plagiarism_results.json')) {
                const data = JSON.parse(fs.readFileSync('plagiarism_results.json', 'utf8'));
                const results = data.detailed_results || [];
                const plagiarismMax = data.plagiarism_max || {};
                const isSingleFolder = data.is_single_folder || false;
                
                let comment = `## 🔍 Plagiarism Check Results\n\n`;
                
                // Add summary
                comment += `### Summary\n\n`;
                comment += `| Item | Plagiarism Percentage |\n|------|----------------------|\n`;
                
                Object.entries(plagiarismMax)
                  .sort((a, b) => b[1] - a[1])  // Sort by percentage (high to low)
                  .forEach(([item, score]) => {
                    const status = score < 30 ? "✅ Low" : score < 70 ? "⚠️ Medium" : "🚫 High";
                    comment += `| ${item} | ${status} (${score}%) |\n`;
                  });
                
                comment += `\n`;
                
                if (results.length > 0) {
                  comment += `### Detailed Matches\n\n`;
                  comment += `⚠️ **Potential plagiarism detected!**\n\n`;
                  comment += `Found ${results.length} match(es) with high similarity:\n\n`;
                  
                  // Group by language for better readability
                  const languageGroups = results.reduce((groups, result) => {
                    const lang = result.language;
                    if (!groups[lang]) groups[lang] = [];
                    groups[lang].push(result);
                    return groups;
                  }, {});
                  
                  for (const [language, langResults] of Object.entries(languageGroups)) {
                    comment += `#### ${language.toUpperCase()}\n`;
                    langResults.forEach(result => {
                      comment += `- Match: ${result.similarity} between ${result.team1} and ${result.team2} (Tool: ${result.tool || 'JPlag'})\n`;
                    });
                    comment += `\n`;
                  }
                } else {
                  comment += `✅ **No detailed plagiarism matches detected!**\n\n`;
                }
                
                comment += `\n*Note: This is an automated check using multiple similarity detection methods. Please review the results manually if needed.*`;
                
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
              } else {
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: '## 🔍 Plagiarism Check\n\nPlagiarism check was executed but no results were found. This could be because there were not enough files to compare or an error occurred during processing.'
                });
              }
            } catch (error) {
              console.error('Error commenting on PR:', error);
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: '## ❌ Plagiarism Check Error\n\nAn error occurred while processing the plagiarism check results.'
              });
            }
