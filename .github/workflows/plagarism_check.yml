name: Plagiarism Check

on:
  push:  # Trigger on any push
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  check-plagiarism:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Java
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 colorama

      - name: Download JPlag
        run: |
          curl -L -o jplag.jar https://github.com/jplag/JPlag/releases/download/v3.0.0/jplag-3.0.0-jar-with-dependencies.jar

      - name: Detect solution folders
        id: detect-folders
        run: |
          # Get all directories (excluding .git and .github)
          FOLDERS=$(find . -maxdepth 1 -type d -not -path "*/\\.*" -not -path "." | sort)
          echo "Found solution folders: $FOLDERS"
          echo "solution_folders=$FOLDERS" >> $GITHUB_OUTPUT

      - name: Check plagiarism with multiple tools
        id: plagiarism-check
        run: |
          # Create a Python script for enhanced plagiarism checking
          cat > check_plagiarism.py << 'EOL'
          import os
          import sys
          import re
          import json
          import subprocess
          from pathlib import Path
          from bs4 import BeautifulSoup
          import requests
          from colorama import Fore, Style, init
          import collections
          import shutil
          import hashlib
          import difflib

          # Initialize colorama
          init()

          # Get all directories in current path (excluding .git and .github)
          solution_folders = [d for d in Path('.').iterdir() 
                              if d.is_dir() and not d.name.startswith('.')]
          
          print(f"Found {len(solution_folders)} solution folders: {[f.name for f in solution_folders]}")
          
          # Define languages to check for with correct JPlag language options
          language_configs = [
              {"name": "python", "extensions": [".py"], "jplag_lang": "python3"},
              {"name": "java", "extensions": [".java"], "jplag_lang": "java"},
              {"name": "javascript", "extensions": [".js", ".jsx"], "jplag_lang": "text"},  # Use text mode for JS
              {"name": "typescript", "extensions": [".ts", ".tsx"], "jplag_lang": "text"},  # Use text mode for TS
              {"name": "cpp", "extensions": [".cpp", ".hpp", ".cc", ".h"], "jplag_lang": "cpp"},
              {"name": "c", "extensions": [".c", ".h"], "jplag_lang": "cpp"},  # Use cpp for C
              {"name": "html", "extensions": [".html", ".htm"], "jplag_lang": "text"},
              {"name": "css", "extensions": [".css", ".scss", ".sass"], "jplag_lang": "text"}
          ]

          # Dictionary to track plagiarism percentages
          file_plagiarism = collections.defaultdict(list)
          team_plagiarism = collections.defaultdict(list)
          results = []

          # Create results directory
          results_dir = Path("plagiarism_results")
          if results_dir.exists():
              shutil.rmtree(results_dir)
          results_dir.mkdir()

          # Function for custom code similarity check
          def check_similarity(file1, file2):
              try:
                  with open(file1, 'r', encoding='utf-8', errors='ignore') as f1, \
                       open(file2, 'r', encoding='utf-8', errors='ignore') as f2:
                      content1 = f1.read()
                      content2 = f2.read()
                      
                      # Remove comments and normalize whitespace to reduce false positives
                      content1 = re.sub(r'\/\/.*|\/\*[\s\S]*?\*\/|#.*', '', content1)
                      content2 = re.sub(r'\/\/.*|\/\*[\s\S]*?\*\/|#.*', '', content2)
                      content1 = re.sub(r'\s+', ' ', content1).strip()
                      content2 = re.sub(r'\s+', ' ', content2).strip()
                      
                      # Skip empty files after comment removal
                      if not content1 or not content2:
                          return 0
                      
                      # Use Python's difflib to compute similarity
                      similarity = difflib.SequenceMatcher(None, content1, content2).ratio() * 100
                      return similarity
              except Exception as e:
                  print(f"{Fore.RED}Error comparing {file1} and {file2}: {e}{Style.RESET_ALL}")
                  return 0

          def get_main_folder_name(path):
              """Get only the top-level folder name from a path"""
              if not path:
                  return "Unknown"
              # Handle strings and Path objects
              parts = path.parts if isinstance(path, Path) else Path(str(path).lstrip('./')).parts
              # Return first non-empty part that's not a hidden folder
              return next((p for p in parts if p and not p.startswith('.')), "Unknown")

          def compare_folders(folder1, folder2, file1, file2):
              """Compare two files and their folders"""
              if folder1 >= folder2:  # Skip self-comparison and duplicates
                  return None
              similarity = check_similarity(file1, file2)
              if similarity >= 30:  # Use 30% threshold
                  return {
                      'similarity': similarity,
                      'folder1': get_main_folder_name(folder1),
                      'folder2': get_main_folder_name(folder2),
                      'file1': str(file1),
                      'file2': str(file2)
                  }
              return None

          # Run plagiarism check for each language
          for lang_config in language_configs:
              lang_name = lang_config["name"]
              lang_extensions = lang_config["extensions"]
              jplag_lang = lang_config["jplag_lang"]
              
              # Find all files of the given language
              language_files = []
              for folder in solution_folders:
                  for ext in lang_extensions:
                      language_files.extend(list(Path(folder).glob(f"**/*{ext}")))
              
              file_count = len(language_files)
              
              if file_count < 2:
                  print(f"{Fore.YELLOW}Not enough {lang_name} files to compare ({file_count} found). Skipping.{Style.RESET_ALL}")
                  continue
              
              print(f"{Fore.CYAN}Checking for plagiarism in {file_count} {lang_name} files...{Style.RESET_ALL}")
              
              # Create output directory for this language
              output_dir = results_dir / lang_name
              output_dir.mkdir(exist_ok=True)
              
              # Method 1: Use JPlag if we have multiple solution folders
              if len(solution_folders) > 1:
                  try:
                      # Run JPlag with correct parameters (no % sign in threshold)
                      cmd = [
                          "java", "-jar", "jplag.jar",
                          "-l", jplag_lang,
                          "-r", str(output_dir),
                          "-m", "0",  # Minimum match length as integer, not percentage
                          "."  # Search in current directory
                      ]
                      
                      # Add language-specific exclude patterns for other languages
                      for exclude_lang in language_configs:
                          if exclude_lang["name"] != lang_name:
                              for ext in exclude_lang["extensions"]:
                                  cmd.extend(["-x", f"**/*{ext}"])
                      
                      # Exclude .git and .github directories
                      cmd.extend(["-x", ".git/**", "-x", ".github/**"])
                      
                      print(f"Running JPlag: {' '.join(cmd)}")
                      process = subprocess.run(cmd, capture_output=True, text=True)
                      
                      if process.returncode != 0:
                          print(f"{Fore.RED}Error running JPlag for {lang_name}:{Style.RESET_ALL}")
                          print(process.stderr)
                      else:
                          print(f"{Fore.GREEN}JPlag completed for {lang_name}{Style.RESET_ALL}")
                          
                          # Parse the results
                          index_file = output_dir / "index.html"
                          if index_file.exists():
                              # Parse the HTML report
                              with open(index_file, "r", encoding="utf-8") as f:
                                  soup = BeautifulSoup(f.read(), 'html.parser')
                              
                              # Extract matches from the table
                              table = soup.find('table')
                              if table:
                                  rows = table.find_all('tr')[1:]  # Skip header row
                                  
                                  for row in rows:
                                      cells = row.find_all('td')
                                      if len(cells) >= 3:
                                          similarity_cell = cells[0]
                                          similarity_text = similarity_cell.get_text(strip=True)
                                          similarity_value = float(similarity_text.replace('%', ''))
                                          
                                          # Get team names from the comparison (only main folder names)
                                          team_links = cells[1:3]
                                          team1 = get_main_folder_name(team_links[0].get_text(strip=True))
                                          team2 = get_main_folder_name(team_links[1].get_text(strip=True))
                                          
                                          # Add to results only if teams are different
                                          if team1 != team2:
                                              # Add to team plagiarism scores
                                              team_plagiarism[team1].append(similarity_value)
                                              team_plagiarism[team2].append(similarity_value)
                                              
                                              # Add to results
                                              results.append({
                                                  'language': lang_name,
                                                  'team1': team1,
                                                  'team2': team2,
                                                  'similarity': f"{similarity_value}%",
                                                  'match_url': f"{lang_name}/jplag_results.html",
                                                  'tool': 'JPlag'
                                              })
                              else:
                                  print(f"{Fore.YELLOW}No JPlag results found for {lang_name}.{Style.RESET_ALL}")
                  except Exception as e:
                      print(f"{Fore.RED}Error in JPlag processing for {lang_name}: {e}{Style.RESET_ALL}")
              
              # Method 2: Custom similarity check (works for both single and multiple folders)
              try:
                  print(f"{Fore.CYAN}Running custom similarity check for {lang_name}...{Style.RESET_ALL}")
                  
                  # Group files by main folder name
                  folder_files = {}
                  for file in language_files:
                      main_folder = get_main_folder_name(file)
                      if main_folder not in folder_files:
                          folder_files[main_folder] = []
                      folder_files[main_folder].append(file)
                  
                  # Compare files between different main folders
                  for folder1, files1 in folder_files.items():
                      for folder2, files2 in folder_files.items():
                          if folder1 >= folder2:  # Skip self-comparison
                              continue
                          
                          for file1 in files1:
                              for file2 in files2:
                                  result = compare_folders(folder1, folder2, file1, file2)
                                  if result:
                                      results.append({
                                          'language': lang_name,
                                          'team1': result['folder1'],
                                          'team2': result['folder2'],
                                          'similarity': f"{result['similarity']:.1f}%",
                                          'tool': 'Custom'
                                      })
                  
                  print(f"{Fore.GREEN}Found matches in custom check for {lang_name}{Style.RESET_ALL}")
              except Exception as e:
                  print(f"{Fore.RED}Error in custom similarity check for {lang_name}: {e}{Style.RESET_ALL}")

          # Calculate the maximum plagiarism percentage for reporting
          plagiarism_max = {}
          
          # For multiple folders, report by team/folder
          if len(solution_folders) > 1:
              for team, scores in team_plagiarism.items():
                  if scores:
                      # Use the maximum plagiarism score found for the team
                      plagiarism_max[team] = max(scores)
                  else:
                      plagiarism_max[team] = 0
          # For single folder, report by file
          else:
              for file_path, scores in file_plagiarism.items():
                  if scores:
                      # Use the maximum plagiarism score found for the file
                      plagiarism_max[file_path] = max(scores)
                  else:
                      plagiarism_max[file_path] = 0

          # Save the results as JSON
          with open("plagiarism_results.json", "w") as f:
              json.dump({
                  'detailed_results': results,
                  'plagiarism_max': plagiarism_max,
                  'is_single_folder': len(solution_folders) == 1
              }, f, indent=2)

          # Create a result URL file for GitHub Actions
          with open("plagiarism_result_url.txt", "w") as f:
              f.write("plagiarism_results/")

          # Print results in the console
          if results:
              print(f"\n{Fore.RED}Found {len(results)} potential plagiarism cases:{Style.RESET_ALL}")
              for result in results:
                  print(f"{Fore.YELLOW}Language: {result['language']} - Match: {result['similarity']} between {result['team1']} and {result['team2']} (Tool: {result['tool']}){Style.RESET_ALL}")
              
              print(f"\n{Fore.CYAN}Plagiarism Summary:{Style.RESET_ALL}")
              for item, score in plagiarism_max.items():
                  status = f"{Fore.RED}HIGH" if score >= 30 else f"{Fore.GREEN}LOW"
                  print(f"{item}: {status} ({score}%){Style.RESET_ALL}")
          else:
              print(f"\n{Fore.GREEN}No plagiarism detected!{Style.RESET_ALL}")
          EOL

          # Run the plagiarism check script
          python check_plagiarism.py || echo "Plagiarism check script execution failed"
          
          # Clean up temporary files
          rm -f check_plagiarism.py

      - name: Check for AI-generated code
        run: |
          # Create a Python script to detect AI-generated code
          cat > detect_ai_code.py << 'EOL'
          import os
          import re
          from pathlib import Path

          def detect_ai_patterns(file_content):
              """Detect patterns that might indicate AI-generated code."""
              ai_patterns = [
                  r"# Generated by ChatGPT",  # Example AI comment
                  r"# AI-generated code",  # Example AI comment
                  r"def .*_ai_.*\(.*\):",  # Function names with '_ai_'
                  r"Generated by OpenAI",  # Common AI tool signature
                  r"This (code|file) was generated by",  # Generic AI generation comment
                  r"<!-- Generated by AI -->",  # HTML comment for AI generation
                  r"\/\* Generated by AI \*\/",  # CSS/JS comment
                  r"// Generated by"  # JS/TS comment
              ]

              for pattern in ai_patterns:
                  if re.search(pattern, file_content, re.IGNORECASE):
                      return True
              return False

          # Scan all solution files
          suspicious_files = []
          for file_path in Path('.').rglob('*.*'):
              if file_path.suffix in ['.py', '.java', '.js', '.jsx', '.ts', '.tsx', '.cpp', '.c', '.h', '.html', '.htm', '.css', '.scss', '.sass']:
                  try:
                      with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                          content = f.read()
                          if detect_ai_patterns(content):
                              suspicious_files.append(str(file_path))
                  except Exception as e:
                      print(f"Error reading {file_path}: {e}")

          # Output results
          if suspicious_files:
              print("\n[WARNING] Potential AI-generated code detected in the following files:")
              for file in suspicious_files:
                  print(f"- {file}")
              with open("ai_detection_results.txt", "w") as f:
                  f.write("\n".join(suspicious_files))
          else:
              print("\n[INFO] No AI-generated code detected.")
          EOL

          # Run the AI detection script
          python detect_ai_code.py || echo "AI detection script execution failed"
          
          # Clean up the temporary script file
          rm -f detect_ai_code.py

      - name: Upload AI detection results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-detection-results
          path: ai_detection_results.txt
          if-no-files-found: ignore

      - name: Upload plagiarism results
        if: always()
        run: |
          # Create empty files if they don't exist to prevent upload failures
          mkdir -p plagiarism_results
          if [ ! -f "plagiarism_result_url.txt" ]; then
            echo "No results available" > plagiarism_result_url.txt
          fi
          if [ ! -f "plagiarism_results.json" ]; then
            echo '{"detailed_results":[],"plagiarism_max":{}}' > plagiarism_results.json
          fi

      - name: Upload results as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: plagiarism-results
          path: |
            plagiarism_result_url.txt
            plagiarism_results.json
            plagiarism_results/
          if-no-files-found: warn

      - name: Update README with plagiarism results
        if: success()
        run: |
          # Create a Python script to update the README
          cat > update_readme.py << 'EOL'
          import json
          import re
          from pathlib import Path
          
          def get_main_folder_name(path):
              """Extract only the main folder name from a path"""
              parts = Path(str(path)).parts
              return next((p for p in parts if p and not p.startswith('.')), path)
          
          # Read and process results
          try:
              with open('plagiarism_results.json', 'r') as f:
                  data = json.load(f)
                  team_results = {}
                  
                  for result in data.get('detailed_results', []):
                      team1 = get_main_folder_name(result.get('team1', ''))
                      team2 = get_main_folder_name(result.get('team2', ''))
                      similarity = float(result.get('similarity', '0').strip('%'))
                      
                      # Update team scores with highest similarity
                      if team1:
                          team_results[team1] = max(similarity, team_results.get(team1, 0))
                      if team2:
                          team_results[team2] = max(similarity, team_results.get(team2, 0))
                          
              # Sort and format results
              plagiarism_section = "## 📊 Plagiarism Analysis\n\n"
              if team_results:
                  sorted_teams = sorted(team_results.items(), key=lambda x: x[1], reverse=True)
                  for team, score in sorted_teams:
                      status = "🚫 HIGH" if score >= 30 else "✅ LOW"
                      plagiarism_section += f"{team} --> {score:.1f}% plagiarism detected ({status})\n\n"
              else:
                  plagiarism_section += "No significant plagiarism detected between teams.\n\n"
              
              # Update README
              with open('README.md', 'r') as f:
                  readme = f.read()
              
              if "## 📊 Plagiarism Analysis" in readme:
                  readme = re.sub(r"## 📊 Plagiarism Analysis\n\n(?:.*\n)*?(?=##|$)", plagiarism_section, readme)
              else:
                  readme += f"\n{plagiarism_section}"
              
              with open('README.md', 'w') as f:
                  f.write(readme)
                  
          except Exception as e:
              print(f"Error updating README: {e}")
          EOL
          
          python update_readme.py
          rm -f update_readme.py

      - name: Commit README changes
        if: success()
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add README.md
          git commit -m "Update README with plagiarism analysis [skip ci]" || echo "No changes to commit"
          git push origin HEAD:${GITHUB_REF_NAME}

      - name: Comment on PR
        if: github.event_name == 'pull_request' && github.actor != 'github-actions[bot]'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            try {
              if (fs.existsSync('plagiarism_results.json')) {
                const data = JSON.parse(fs.readFileSync('plagiarism_results.json', 'utf8'));
                const results = data.detailed_results || [];
                
                // Aggregate results by team
                const teamResults = {};
                for (const result of results) {
                  const team1 = result.team1.split('/')[0];
                  const team2 = result.team2.split('/')[0];
                  const similarity = parseFloat(result.similarity);
                  
                  if (!teamResults[team1] || similarity > teamResults[team1]) {
                    teamResults[team1] = similarity;
                  }
                  if (!teamResults[team2] || similarity > teamResults[team2]) {
                    teamResults[team2] = similarity;
                  }
                }
                
                let comment = `## 🔍 Plagiarism Analysis Results\n\n`;
                
                // Sort teams by plagiarism percentage
                const sortedTeams = Object.entries(teamResults)
                  .sort((a, b) => b[1] - a[1]);
                
                if (sortedTeams.length > 0) {
                  sortedTeams.forEach(([team, score]) => {
                    const status = score >= 30 ? "🚫 HIGH" : "✅ LOW";
                    comment += `${team} --> ${score.toFixed(1)}% plagiarism detected (${status})\n\n`;
                  });
                } else {
                  comment += `✅ No significant plagiarism detected between teams.\n\n`;
                }
                
                comment += `\n*Note: This is an automated analysis. Please review the results manually if needed.*`;
                
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
              } else {
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: '## 🔍 Plagiarism Check\n\nNo results found. This could be because there were not enough files to compare.'
                });
              }
            } catch (error) {
              console.error('Error commenting on PR:', error);
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: '## ❌ Plagiarism Check Error\n\nAn error occurred while processing the plagiarism results.'
              });
            }
