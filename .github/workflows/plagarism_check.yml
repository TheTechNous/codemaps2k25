name: Plagiarism Check

on:
  push:  # Trigger on any push
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  check-plagiarism:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}  # Use token for proper permissions

      - name: Set up Java
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 colorama

      - name: Download JPlag
        run: |
          curl -L -o jplag.jar https://github.com/jplag/JPlag/releases/download/v3.0.0/jplag-3.0.0-jar-with-dependencies.jar
          # Verify the download succeeded
          if [ ! -f jplag.jar ]; then
            echo "Failed to download JPlag jar file"
            exit 1
          fi
          echo "Successfully downloaded JPlag jar file"
          ls -la jplag.jar

      - name: Detect solution folders
        id: detect-folders
        run: |
          # Get all directories (excluding .git and .github) with proper escaping
          FOLDERS=$(find . -maxdepth 1 -type d -not -path "*/\.*" -not -path "." | sed 's/^\.\///' | sort)
          echo "Found solution folders: $FOLDERS"
          # Escape any special characters in folder names
          SAFE_FOLDERS=$(echo "$FOLDERS" | tr '\n' ' ' | sed 's/[^a-zA-Z0-9_ ]/_/g')
          echo "solution_folders=$SAFE_FOLDERS" >> $GITHUB_OUTPUT

      - name: Check plagiarism with multiple tools
        id: plagiarism-check
        run: |
          # Create a Python script for enhanced plagiarism checking
          cat > check_plagiarism.py << 'EOL'
          import os
          import sys
          import re
          import json
          import subprocess
          from pathlib import Path
          from bs4 import BeautifulSoup
          import requests
          from colorama import Fore, Style, init
          import collections
          import shutil
          import hashlib
          import difflib
          import tempfile

          # Initialize colorama
          init()

          def normalize_path(path):
              """Normalize path to handle both Windows and Unix paths and any format"""
              if isinstance(path, Path):
                  path = str(path)
              # Remove any leading ./ or .\ and normalize slashes
              path = re.sub(r'^\.[\\/]', '', str(path))
              return path.replace('\\', '/')

          def sanitize_folder_name(name):
              """Sanitize folder name to prevent format errors"""
              if not name:
                  return "Unknown"
              
              # Remove any characters that might cause issues
              safe_name = re.sub(r'[^a-zA-Z0-9_\-.]', '_', str(name))
              return safe_name if safe_name else "Unknown"

          def get_main_folder_name(path):
              """Extract only the main folder name from a path, handling any format"""
              if not path or isinstance(path, (int, float, bool)):
                  return "Unknown"
              
              # Convert to string and normalize
              path_str = str(path)
              # Remove leading ./ or .\ and split
              clean_path = re.sub(r'^\.[\\/]', '', path_str)
              # Split by both forward and backward slashes
              parts = re.split(r'[\\/]', clean_path)
              # Get first non-empty part that's not a dot
              for part in parts:
                  if part and not part.startswith('.'):
                      return sanitize_folder_name(part)
              return "Unknown"

          def process_file_path(path):
              """Process and validate file path"""
              try:
                  if not path:
                      return None
                  # Convert to string if it's a Path object
                  path_str = str(path)
                  # Remove any invalid characters and normalize slashes
                  clean_path = re.sub(r'[<>:"|?*]', '', path_str)
                  return clean_path.replace('\\', '/')
              except Exception:
                  return None

          def get_file_list(directory):
              """Get list of files while handling path errors"""
              try:
                  if isinstance(directory, str):
                      directory = Path(directory)
                  return [f for f in directory.rglob('*') if f.is_file()]
              except Exception as e:
                  print(f"{Fore.RED}Error accessing directory {directory}: {e}{Style.RESET_ALL}")
                  return []

          def compare_folders(folder1, folder2, file1, file2):
              """Compare files from different folders and return results if similarity threshold is met"""
              try:
                  if folder1 == folder2:  # Skip if same folder
                      return None
                  
                  similarity = check_similarity(file1, file2)
                  if similarity >= 30:  # Using 30% threshold
                      return {
                          'folder1': folder1,
                          'folder2': folder2,
                          'file1': str(file1),
                          'file2': str(file2),
                          'similarity': similarity
                      }
                  return None
              except Exception as e:
                  print(f"{Fore.RED}Error comparing folders {folder1} and {folder2}: {e}{Style.RESET_ALL}")
                  return None

          def get_extensions_for_language(lang_name):
              """Get file extensions for a given language"""
              for config in language_configs:
                  if config['name'] == lang_name:
                      return config['extensions']
              return []

          # Get all directories in current path (excluding .git and .github)
          solution_folders = [d for d in Path('.').iterdir() 
                            if d.is_dir() and not d.name.startswith('.')]

          print(f"Found {len(solution_folders)} solution folders: {[f.name for f in solution_folders]}")
          
          # Define languages to check for with correct JPlag language options
          language_configs = [
              {"name": "python", "extensions": [".py"], "jplag_lang": "python3"},
              {"name": "java", "extensions": [".java"], "jplag_lang": "java"},
              {"name": "javascript", "extensions": [".js", ".jsx"], "jplag_lang": "text"},  # Use text mode for JS
              {"name": "typescript", "extensions": [".ts", ".tsx"], "jplag_lang": "text"},  # Use text mode for TS
              {"name": "cpp", "extensions": [".cpp", ".hpp", ".cc", ".h"], "jplag_lang": "cpp"},
              {"name": "c", "extensions": [".c", ".h"], "jplag_lang": "cpp"},  # Use cpp for C
              {"name": "html", "extensions": [".html", ".htm"], "jplag_lang": "text"},
              {"name": "css", "extensions": [".css", ".scss", ".sass"], "jplag_lang": "text"}
          ]

          # Dictionary to track plagiarism percentages
          file_plagiarism = collections.defaultdict(list)
          team_plagiarism = collections.defaultdict(list)
          results = []

          # Create results directory
          results_dir = Path("plagiarism_results")
          if results_dir.exists():
              shutil.rmtree(results_dir)
          results_dir.mkdir()

          # Function for custom code similarity check
          def check_similarity(file1, file2):
              try:
                  file1 = normalize_path(file1)
                  file2 = normalize_path(file2)
                  
                  with open(file1, 'r', encoding='utf-8', errors='ignore') as f1, \
                       open(file2, 'r', encoding='utf-8', errors='ignore') as f2:
                      content1 = f1.read()
                      content2 = f2.read()
                      
                      # Remove comments and normalize whitespace
                      content1 = re.sub(r'\/\/.*|\/\*[\s\S]*?\*\/|#.*', '', content1)
                      content2 = re.sub(r'\/\/.*|\/\*[\s\S]*?\*\/|#.*', '', content2)
                      content1 = re.sub(r'\s+', ' ', content1).strip()
                      content2 = re.sub(r'\s+', ' ', content2).strip()
                      
                      if not content1 or not content2:
                          return 0
                      
                      similarity = difflib.SequenceMatcher(None, content1, content2).ratio() * 100
                      return similarity
              except Exception as e:
                  print(f"{Fore.RED}Error comparing {file1} and {file2}: {e}{Style.RESET_ALL}")
                  return 0

          # Run plagiarism check for each language
          for lang_config in language_configs:
              lang_name = lang_config["name"]
              lang_extensions = lang_config["extensions"]
              jplag_lang = lang_config["jplag_lang"]
              
              # Find all files of the given language using normalized paths
              language_files = []
              for folder in solution_folders:
                  try:
                      folder_path = process_file_path(folder)
                      if folder_path:
                          for ext in lang_extensions:
                              # Use rglob to find all matching files
                              found_files = Path(folder_path).rglob(f"*{ext}")
                              language_files.extend([f for f in found_files if f.is_file()])
                  except Exception as e:
                      print(f"{Fore.RED}Error processing folder {folder}: {e}{Style.RESET_ALL}")
                      continue

              file_count = len(language_files)
              
              if file_count < 2:
                  print(f"{Fore.YELLOW}Not enough {lang_name} files to compare ({file_count} found). Skipping.{Style.RESET_ALL}")
                  continue
              
              print(f"{Fore.CYAN}Checking for plagiarism in {file_count} {lang_name} files...{Style.RESET_ALL}")
              
              # Create output directory for this language
              output_dir = results_dir / lang_name
              output_dir.mkdir(exist_ok=True)
              
              # Method 1: Use JPlag if we have multiple solution folders
              if len(solution_folders) > 1:
                  try:
                      # Create separate directories for each language to avoid conflicts
                      lang_dir = Path("temp_jplag_files") / lang_name
                      if lang_dir.exists():
                          shutil.rmtree(lang_dir)
                      lang_dir.mkdir(parents=True, exist_ok=True)
                      
                      # Keep track of valid teams for JPlag
                      valid_teams = set()
                      
                      # Copy only relevant files to the language directory
                      for file in language_files:
                          try:
                              # Create destination folder maintaining relative structure
                              rel_path = file.relative_to(Path('.'))
                              team_folder = get_main_folder_name(file)
                              
                              # Ensure the team name is valid for JPlag (no spaces or special chars)
                              safe_team = re.sub(r'[^a-zA-Z0-9_]', '_', team_folder)
                              
                              # Create team subdirectory
                              team_dir = lang_dir / safe_team
                              team_dir.mkdir(parents=True, exist_ok=True)
                              
                              # Create a unique filename to avoid collisions
                              file_hash = hashlib.md5(str(file).encode()).hexdigest()[:8]
                              dest_file = team_dir / f"{rel_path.stem}_{file_hash}{rel_path.suffix}"
                              
                              # Copy the file
                              shutil.copy2(file, dest_file)
                              valid_teams.add(safe_team)
                              
                          except Exception as e:
                              print(f"{Fore.RED}Error copying file {file}: {e}{Style.RESET_ALL}")
                              continue
                      
                      # Skip JPlag if we don't have enough valid teams
                      if len(valid_teams) < 2:
                          print(f"{Fore.YELLOW}Not enough valid teams for {lang_name} (need at least 2). Skipping JPlag.{Style.RESET_ALL}")
                          continue
                          
                      # Run JPlag with correct parameters
                      cmd = [
                          "java", "-jar", "jplag.jar",
                          "-l", jplag_lang,
                          "-r", str(output_dir),
                          "-m", "0",  # Minimum match length
                          str(lang_dir)  # Input directory containing all team folders
                      ]
                      
                      # Print the exact command that will be run
                      cmd_str = ' '.join(cmd)
                      print(f"{Fore.CYAN}Checking for plagiarism in {file_count} {lang_name} files across {len(valid_teams)} teams...{Style.RESET_ALL}")
                      print(f"Running JPlag: {cmd_str}")
                      
                      # Run JPlag with verbose output
                      process = subprocess.run(cmd, capture_output=True, text=True)
                      
                      # Always print output for debugging
                      print(f"JPlag stdout: {process.stdout}")
                      print(f"JPlag stderr: {process.stderr}")
                      
                      if process.returncode != 0:
                          print(f"{Fore.RED}Error running JPlag for {lang_name}:{Style.RESET_ALL}")
                          print(f"Exit code: {process.returncode}")
                          # Fall back to custom similarity check
                          print(f"{Fore.YELLOW}Falling back to custom similarity check for {lang_name}{Style.RESET_ALL}")
                      else:
                          print(f"{Fore.GREEN}JPlag completed for {lang_name}{Style.RESET_ALL}")
                          
                          # Parse the results - look in the specific language output directory
                          result_dirs = list(output_dir.glob("*"))
                          if result_dirs:
                              result_dir = result_dirs[0]  # Use first result directory
                              index_file = result_dir / "index.html"
                              
                              if index_file.exists():
                                  # Parse the HTML report
                                  with open(index_file, "r", encoding="utf-8") as f:
                                      soup = BeautifulSoup(f.read(), 'html.parser')
                                  
                                  # Extract matches from the table
                                  table = soup.find('table')
                                  if table:
                                      rows = table.find_all('tr')[1:]  # Skip header row
                                      
                                      for row in rows:
                                          cells = row.find_all('td')
                                          if len(cells) >= 3:
                                              similarity_cell = cells[0]
                                              similarity_text = similarity_cell.get_text(strip=True)
                                              similarity_value = float(similarity_text.replace('%', ''))
                                              
                                              # Get team names from the comparison (only main folder names)
                                              team_links = cells[1:3]
                                              team1 = get_main_folder_name(team_links[0].get_text(strip=True))
                                              team2 = get_main_folder_name(team_links[1].get_text(strip=True))
                                              
                                              # Add to results only if teams are different
                                              if team1 != team2:
                                                  # Add to team plagiarism scores
                                                  team_plagiarism[team1].append(similarity_value)
                                                  team_plagiarism[team2].append(similarity_value)
                                                  
                                                  # Add to results
                                                  results.append({
                                                      'language': lang_name,
                                                      'team1': team1,
                                                      'team2': team2,
                                                      'similarity': f"{similarity_value}%",
                                                      'match_url': f"{lang_name}/jplag_results.html",
                                                      'tool': 'JPlag'
                                                  })
                                  else:
                                      print(f"{Fore.YELLOW}No JPlag results found for {lang_name}.{Style.RESET_ALL}")
                  except Exception as e:
                      print(f"{Fore.RED}Error in JPlag processing for {lang_name}: {e}{Style.RESET_ALL}")
              
              # Method 2: Custom similarity check (works for both single and multiple folders)
              try:
                  print(f"{Fore.CYAN}Running custom similarity check for {lang_name}...{Style.RESET_ALL}")
                  
                  # Group files by main folder name
                  folder_files = {}
                  for file in language_files:
                      try:
                          main_folder = get_main_folder_name(file)
                          if main_folder not in folder_files:
                              folder_files[main_folder] = []
                          folder_files[main_folder].append(file)
                      except Exception as e:
                          print(f"{Fore.RED}Error processing file path {file}: {e}{Style.RESET_ALL}")
                          continue
                  
                  # Compare files between different main folders
                  for folder1, files1 in folder_files.items():
                      for folder2, files2 in folder_files.items():
                          if folder1 >= folder2:  # Skip self-comparison
                              continue
                          
                          for file1 in files1:
                              for file2 in files2:
                                  result = compare_folders(folder1, folder2, file1, file2)
                                  if result:
                                      results.append({
                                          'language': lang_name,
                                          'team1': result['folder1'],
                                          'team2': result['folder2'],
                                          'similarity': f"{result['similarity']:.1f}%",
                                          'tool': 'Custom'
                                      })
                  
                  print(f"{Fore.GREEN}Found matches in custom check for {lang_name}{Style.RESET_ALL}")
              except Exception as e:
                  print(f"{Fore.RED}Error in custom similarity check for {lang_name}: {e}{Style.RESET_ALL}")

          # Calculate the maximum plagiarism percentage for reporting
          plagiarism_max = {}
          
          # For multiple folders, report by team/folder
          if len(solution_folders) > 1:
              for team, scores in team_plagiarism.items():
                  if scores:
                      # Use the maximum plagiarism score found for the team
                      plagiarism_max[team] = max(scores)
                  else:
                      plagiarism_max[team] = 0
          # For single folder, report by file
          else:
              for file_path, scores in file_plagiarism.items():
                  if scores:
                      # Use the maximum plagiarism score found for the file
                      plagiarism_max[file_path] = max(scores)
                  else:
                      plagiarism_max[file_path] = 0

          # Save results with extra path handling and sanitization
          with open("plagiarism_results.json", "w") as f:
              normalized_results = []
              for result in results:
                  try:
                      normalized_result = result.copy()
                      if 'team1' in result:
                          normalized_result['team1'] = sanitize_folder_name(result['team1'])
                      if 'team2' in result:
                          normalized_result['team2'] = sanitize_folder_name(result['team2'])
                      if 'file1' in result:
                          path1 = process_file_path(result['file1'])
                          normalized_result['file1'] = path1 if path1 else "unknown_file1"
                      if 'file2' in result:
                          path2 = process_file_path(result['file2'])
                          normalized_result['file2'] = path2 if path2 else "unknown_file2"
                      normalized_results.append(normalized_result)
                  except Exception as e:
                      print(f"{Fore.RED}Error normalizing result: {e}{Style.RESET_ALL}")
                      continue
              
              # Make sure to handle any potential errors in the final results
              try:
                  json.dump({
                      'detailed_results': normalized_results,
                      'plagiarism_max': {sanitize_folder_name(k): v for k, v in plagiarism_max.items()},
                      'is_single_folder': len(solution_folders) == 1
                  }, f, indent=2)
              except Exception as e:
                  print(f"{Fore.RED}Error writing results to JSON: {e}{Style.RESET_ALL}")
                  # Fallback to a simple valid JSON if the original data has issues
                  json.dump({
                      'detailed_results': [],
                      'plagiarism_max': {},
                      'is_single_folder': len(solution_folders) == 1
                  }, f, indent=2)

          # Create a result URL file for GitHub Actions
          with open("plagiarism_result_url.txt", "w") as f:
              f.write("plagiarism_results/")

          # Print results in the console
          if results:
              print(f"\n{Fore.RED}Found {len(results)} potential plagiarism cases:{Style.RESET_ALL}")
              for result in results:
                  print(f"{Fore.YELLOW}Language: {result['language']} - Match: {result['similarity']} between {result['team1']} and {result['team2']} (Tool: {result['tool']}){Style.RESET_ALL}")
              
              print(f"\n{Fore.CYAN}Plagiarism Summary:{Style.RESET_ALL}")
              for item, score in plagiarism_max.items():
                  status = f"{Fore.RED}HIGH" if score >= 30 else f"{Fore.GREEN}LOW"
                  print(f"{item}: {status} ({score}%){Style.RESET_ALL}")
          else:
              print(f"\n{Fore.GREEN}No plagiarism detected!{Style.RESET_ALL}")
          EOL

          # Run the plagiarism check script
          python check_plagiarism.py || echo "Plagiarism check script execution failed"
          
          # Clean up temporary files
          rm -f check_plagiarism.py

      - name: Check for AI-generated code
        run: |
          # Create a Python script to detect AI-generated code
          cat > detect_ai_code.py << 'EOL'
          import os
          import re
          from pathlib import Path

          def detect_ai_patterns(file_content):
              """Detect patterns that might indicate AI-generated code."""
              ai_patterns = [
                  r"# Generated by ChatGPT",  # Example AI comment
                  r"# AI-generated code",  # Example AI comment
                  r"def .*_ai_.*\(.*\):",  # Function names with '_ai_'
                  r"Generated by OpenAI",  # Common AI tool signature
                  r"This (code|file) was generated by",  # Generic AI generation comment
                  r"<!-- Generated by AI -->",  # HTML comment for AI generation
                  r"\/\* Generated by AI \*\/",  # CSS/JS comment
                  r"// Generated by"  # JS/TS comment
              ]

              for pattern in ai_patterns:
                  if re.search(pattern, file_content, re.IGNORECASE):
                      return True
              return False

          # Scan all solution files
          suspicious_files = []
          for file_path in Path('.').rglob('*.*'):
              if file_path.suffix in ['.py', '.java', '.js', '.jsx', '.ts', '.tsx', '.cpp', '.c', '.h', '.html', '.htm', '.css', '.scss', '.sass']:
                  try:
                      with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                          content = f.read()
                          if detect_ai_patterns(content):
                              suspicious_files.append(str(file_path))
                  except Exception as e:
                      print(f"Error reading {file_path}: {e}")

          # Output results
          if suspicious_files:
              print("\n[WARNING] Potential AI-generated code detected in the following files:")
              for file in suspicious_files:
                  print(f"- {file}")
              with open("ai_detection_results.txt", "w") as f:
                  f.write("\n".join(suspicious_files))
          else:
              print("\n[INFO] No AI-generated code detected.")
          EOL

          # Run the AI detection script
          python detect_ai_code.py || echo "AI detection script execution failed"
          
          # Clean up the temporary script file
          rm -f detect_ai_code.py

      - name: Upload AI detection results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-detection-results
          path: ai_detection_results.txt
          if-no-files-found: ignore

      - name: Upload plagiarism results
        if: always()
        run: |
          # Create empty files if they don't exist to prevent upload failures
          mkdir -p plagiarism_results
          if [ ! -f "plagiarism_result_url.txt" ]; then
            echo "No results available" > plagiarism_result_url.txt
          fi
          if [ ! -f "plagiarism_results.json" ]; then
            echo '{"detailed_results":[],"plagiarism_max":{}}' > plagiarism_results.json
          fi

      - name: Upload results as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: plagiarism-results
          path: |
            plagiarism_result_url.txt
            plagiarism_results.json
            plagiarism_results/
          if-no-files-found: warn

      - name: Update README with plagiarism results
        if: success()
        run: |
          # Create a Python script to update the README
          cat > update_readme.py << 'EOL'
          import json
          import re
          from pathlib import Path
          
          def get_main_folder_name(path):
              """Extract only the main folder name from a path"""
              parts = Path(str(path)).parts
              return next((p for p in parts if p and not p.startswith('.')), path)
          
          # Read and process results
          try:
              with open('plagiarism_results.json', 'r') as f:
                  data = json.load(f)
                  team_results = {}
                  
                  for result in data.get('detailed_results', []):
                      team1 = get_main_folder_name(result.get('team1', ''))
                      team2 = get_main_folder_name(result.get('team2', ''))
                      similarity = float(result.get('similarity', '0').strip('%'))
                      
                      # Update team scores with highest similarity
                      if team1:
                          team_results[team1] = max(similarity, team_results.get(team1, 0))
                      if team2:
                          team_results[team2] = max(similarity, team_results.get(team2, 0))
                          
              # Sort and format results
              plagiarism_section = "## 📊 Plagiarism Analysis\n\n"
              if team_results:
                  sorted_teams = sorted(team_results.items(), key=lambda x: x[1], reverse=True)
                  for team, score in sorted_teams:
                      status = "🚫 HIGH" if score >= 30 else "✅ LOW"
                      plagiarism_section += f"{team} --> {score:.1f}% plagiarism detected ({status})\n\n"
              else:
                  plagiarism_section += "No significant plagiarism detected between teams.\n\n"
              
              # Update README
              with open('README.md', 'r') as f:
                  readme = f.read()
              
              if "## 📊 Plagiarism Analysis" in readme:
                  readme = re.sub(r"## 📊 Plagiarism Analysis\n\n(?:.*\n)*?(?=##|$)", plagiarism_section, readme)
              else:
                  readme += f"\n{plagiarism_section}"
              
              with open('README.md', 'w') as f:
                  f.write(readme)
                  
          except Exception as e:
              print(f"Error updating README: {e}")
          EOL
          
          python update_readme.py
          rm -f update_readme.py

      - name: Commit README changes
        if: success()
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add README.md
          git commit -m "Update README with plagiarism analysis [skip ci]" || echo "No changes to commit"
          
          # Pull before pushing to handle remote changes
          git pull --rebase origin ${GITHUB_REF_NAME} || echo "Failed to pull - will attempt to push anyway"
          
          # Try to push, if it fails still, add more force
          git push origin HEAD:${GITHUB_REF_NAME} || git push --force-with-lease origin HEAD:${GITHUB_REF_NAME}

      - name: Comment on PR
        if: github.event_name == 'pull_request' && github.actor != 'github-actions[bot]'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            try {
              if (fs.existsSync('plagiarism_results.json')) {
                const data = JSON.parse(fs.readFileSync('plagiarism_results.json', 'utf8'));
                const results = data.detailed_results || [];
                
                // Aggregate results by team
                const teamResults = {};
                for (const result of results) {
                  const team1 = result.team1.split('/')[0];
                  const team2 = result.team2.split('/')[0];
                  const similarity = parseFloat(result.similarity);
                  
                  if (!teamResults[team1] || similarity > teamResults[team1]) {
                    teamResults[team1] = similarity;
                  }
                  if (!teamResults[team2] || similarity > teamResults[team2]) {
                    teamResults[team2] = similarity;
                  }
                }
                
                let comment = `## 🔍 Plagiarism Analysis Results\n\n`;
                
                // Sort teams by plagiarism percentage
                const sortedTeams = Object.entries(teamResults)
                  .sort((a, b) => b[1] - a[1]);
                
                if (sortedTeams.length > 0) {
                  sortedTeams.forEach(([team, score]) => {
                    const status = score >= 30 ? "🚫 HIGH" : "✅ LOW";
                    comment += `${team} --> ${score.toFixed(1)}% plagiarism detected (${status})\n\n`;
                  });
                } else {
                  comment += `✅ No significant plagiarism detected between teams.\n\n`;
                }
                
                comment += `\n*Note: This is an automated analysis. Please review the results manually if needed.*`;
                
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
              } else {
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: '## 🔍 Plagiarism Check\n\nNo results found. This could be because there were not enough files to compare.'
                });
              }
            } catch (error) {
              console.error('Error commenting on PR:', error);
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: '## ❌ Plagiarism Check Error\n\nAn error occurred while processing the plagiarism results.'
              });
            }
